---
layout: post
title: "Some Bug Balance"
---

There is a notion of **Zero Bug Balance** in my department. In theory, Zero Bug Balance should increase our software quality.

Periodically, we:

* Count bugs found
* Count bugs fixed
* Apply a top secret formula: `bug_balance[this_time] = bugs_found - bugs_fixed + bug_balance[last_time]`
* If `bug_balance[this_time] > 0`, that's kind of bad.
* If `bug_balance[this_time] > bug_balance[last_time]`, that's bad for real.

However, judging software quality from this metric could be improved.

## If I cite a memetic law I must be legit

Let's get this over with: I'll choose Goodhart's Law, which has come to be interpreted as:

> "When a measure becomes a target, it ceases to be a good measure."

Zero bug balance as a scalar metric promotes an unhealthy work culture.

With Zero Bug Balance...
* reporting bugs looks bad
* fixing bugs looks good
* keeping bugs looks bad

To our department's credit, nothing very bad happens if we increase our bug balance, aside from the public shame of admitting we created more than zero bugs (sometimes a lot more). This shame can compulsively drive one to reduce said `bug_balance` to something closer to `0`.

As a software engineer, I've felt that shame of having to file a bug. In fact, my manager has cautioned us before about labeling issues as "bugs," because we're chasing Zero Bug Balance, and `last_time` we had bad numbers.

Frankly, I have the luxury of both exceptionally empathetic management and being a cranky, half-white, sometimes-loud, male software engineer who does not give a crap about whether some metric dubbed "bug balance" is too high. But many (most?) others _do care_ and/or _do not feel comfortable_ breaking ranks or spoiling team metrics, even at the cost of their product.

Less-spoiled engineers who do not trust their leadership have incentive to hide bugs for the sake of their project looking successful. However, which would you trust more: a project that found and fixed zero bugs, or one that found a dozen bugs and fixed half?

And wait, half? Why would we ever want to keep a bug? Zero Bug Balance has no place for this mindset.

## Bugs are not unidimensional

Treating a bug like a boolean (does it exist or not?) isn't just dangerous, it's _boring_. Bugs have character because they carry inherent severity.

But what makes a bug "severe?" In my opinion, whatever makes it scary.

I have an arsenal of negative experiences with  [SAFe](https://en.wikipedia.org/wiki/Scaled_agile_framework), but for once I'll share something _good_ that resulted from my time with it: we learned risk assesment 101. Issues (stories? cards?) were cast onto a 2-dimensional risk matrix I still begrudgingly consider when defining bug **severity**:

* **probability**: likelihood or frequency that a bug occurs.
* **impact**: how awful things get because of the bug.

Naturally, two axes and four quadrants lay themselves out. Product managers rejoice, a new mechanism for categorizing otherwise intangible things appears on a whiteboard, and engineers groan about contending with yet another construct they can't write unit tests for. Each side's sentiment annihilates its counterpart, and the room achieves zero enthusiasm balance.

But here suddenly each bug is no longer just an existence boolean, its character builds with every question:

* How often does this bug come up?
* How long will this bug exist? How long has it existed?
* Was this bug the result of bad implementation? Poor design? Questionable architecture?
* What will it take to rid our code of this bug? Is this worth the effort right now?
* How much sleep will I lose over this bug hiding? Happening?

Bug severity quantifies how much fear it strikes into each engineer's heart. Left to any team that values sleep, prioritizing fixes for bugs scary enough (sufficient `probability * impact`) to keep them up at night will occur naturally.

## Technical Debt and Opportunity Cost

Financial analogies, financial analogies everywhere!

Technical debt comes in many flavors. I'm definitely guilty of creating several: releasing things without automated tests, deciding on an architecture without documentation, hoarding team knowledge (like interest, this gets worse over time if unmitigated), and maintaining an average [bus factor](https://en.wikipedia.org/wiki/Bus_factor) of slightly less than 1 (it's possible). Most of these warrant separate discussion, but all of these can lead to bugs.

Fixing any of these bugs--or better yet, their causes-- takes time and effort, which may detract from team priorities at the given moment.

A visual bug in a UI probably matters less than a data corruption bug in an API. Similarly, a bug in a component may not be worth fixing if a new component will replace it entirely. Or perhaps a bug is worth fixing _sometime_, except the person representing the entire bus factor for that part is on vacation.

Teams must weigh the opportunity cost of when to solve a bug (if ever), considering other priorities. [Erik Bernhardsson describes this scenario, as seen by outsiders:](https://erikbern.com/2020/03/10/never-attribute-to-stupidity-that-which-is-adequately-explained-by-opportunity-cost.html)

> Why is _bug Z_ still present? Hasn't it been known for a really long time? I don't understand why they aren't fixing it?

And his answer:

> Of course, why these things never happened is that _something else was more important._

Zero Bug Balance does not account for _acceptably_ small bugs, encourages disregard for opportunity cost, and discourages the team from leveraging technical debt to choose between shipping features or improving quality.

## What metric would work better?

### Zero Bug Bounce

The best summary I could find of Microsoft's Zero Bug Bounce is [Mike Torres' opening paragraph relating Zero Bug Bounce to life](http://www.refocuser.com/2009/04/bouncing-at-zero-zbb-in-life/):

> all active bugs in the software have been looked at and either punted or fixed – and the team’s fix rate (or the rate at which they’re able to fix bugs) is greater than the team’s incoming rate (or the rate at which new bugs are being opened).

This is the dream! Bugs are either handled or punted. Unfortuantely, Zero Bug Balance measures currently-punted bugs. Zero Bug Bounce, on the other hand, measures the rate at which new bugs are created vs team capacity.

Additionally,

> The code is almost ready to see the light of day in the real world.

... I'm not sure is this applies as well to modern deployed software.

## Track bugs with severity

Simple in concept: attach a severity to every bug to report how scary it is.

In practice, severity is useful for prioritizing bugs, and that's it. No one outside the team actually cares about _bug_ severity, they care about _incident_ severity.

Naturally, having management wrestle with questions like "is the team that fixed 40 low-severity bugs doing better than the team that only fixed 1 high-severity bug?" and this quickly becomes a useless metric by itself.

### The Product Management Dream Method for Bug Measurementificationalization (PMDMFBM)

In a world where product management is enlisted to manage bugs:
* Each bug is given a score based on its risk and impact
    * Edge cases don't exist: No bug will ever fall outside the rubric
* Severity score is calculated according to an insane algorithm involving everything up to single variable calculus.
    * Don't worry, PM has created a wizard to this in Excel, with only a few manual edits
* Manually sync all bugs to JIRA.
* So we're all on the same page, ever update pages the entire team.

_This is a joke, no one wants this_


## BFFs: Bugs Found and Fixed

```
BFF = bugs_found + bugs_fixed
```
I realize I just spent an entire article denouncing a single metric, but realistically, a single number is easiest to report, if we keep in mind other better delivery practices.

In my opinion, this is a better alternative to Zero Bug Balance:

* Process-wise, these numbers are easy to report.
* Culturally, _finding_ a bug carries the same weight as _fixing_ (closing) a bug.

This expects engineers to meet the following expectations:

* Continually track and revisit bugs, prioritizing by severity.
* Weigh the opportunity costs of feature development vs paying down technical debt 

Management 

* Wave around BFFs as the single bug metric. Care a little less about bug balances.
* Acknowledge that teams may take on technical debt.
* **Measure other things that promote better delivery,** like shipping frequency or mean time to recovery.

BFFs are still just one metric in measuring software quality. It's not perfect, nor is it comprehensive, but that's the point: it's a metric that remains conducive to better delivery practices.

Plus, who wouldn't want a BFF?
